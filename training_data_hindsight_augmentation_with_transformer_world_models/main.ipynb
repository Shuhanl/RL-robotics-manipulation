{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEcuSBdHQpfd",
        "outputId": "2a641c10-8d86-468b-c052-441c1ca9fe35"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtuK2i1jQ4nc",
        "outputId": "8cace65a-159f-4e35-d62a-1481f08d0007"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "from tianshou.data import Batch, ReplayBuffer, SegmentTree, to_numpy\n",
        "\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "import imageio\n",
        "from base64 import b64encode\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjX3oBFf5YpY",
        "outputId": "18f2eb25-47b9-4ff2-c1cf-b39f6bd5b2e5"
      },
      "outputs": [],
      "source": [
        "from learner import Learner\n",
        "from seq_trainer import SequenceTrainer\n",
        "from env import Stack\n",
        "\n",
        "class SerialModelTrain():\n",
        "  def __init__(self):\n",
        "    self.num_episode = 10\n",
        "    self.initial_memory_size = 20\n",
        "    self.memory_size = 1024\n",
        "    self.episode_rewards = []\n",
        "    self.num_average_epidodes = 100\n",
        "    self.save_every = 100\n",
        "    self.batch_size=3072\n",
        "    self.max_steps = 100\n",
        "    self.max_iterators = 10\n",
        "    self.env = Stack('Panda') \n",
        "    self.robot_state_shape = 7\n",
        "    self.action_shape = self.env.robots[0].robot_model.dof + self.env.robots.gripper.dof\n",
        "\n",
        "    self.learner = Learner(self.action_shape, self.robot_state_shape, self.memory_size)\n",
        "    self.trainer = SequenceTrainer(batch_size=self.batch_size)\n",
        "\n",
        "    self.evaluate_interval = 10\n",
        "    self.num_steps_per_iter = 10000\n",
        "    self.reward_window = 20\n",
        "    self.reward_values = [0]*self.reward_window \n",
        "    self.averge_reward = 0\n",
        "\n",
        "  def _imagePreProcess_(self, image):\n",
        "    image = torch.tensor(image.transpose(2, 0, 1), dtype=torch.float32).unsqueeze(0)\n",
        "    image = 0.2989 * image[:, 0, :, :] + 0.5870 * image[:, 1, :, :] + 0.1140 * image[:, 2, :, :]\n",
        "    image = image.unsqueeze(1)\n",
        "    image = F.interpolate(image, size=(64, 64), mode='bilinear', align_corners=False)\n",
        "    return image\n",
        "\n",
        "  def init_buffer(self):\n",
        "    \"\"\" Initially, put the data into the replay buffer when an action with noise was taken \"\"\"\n",
        "    state = OrderedDict() \n",
        "    next_state = OrderedDict() \n",
        "    done = False \n",
        "    reward = 0\n",
        "\n",
        "    state = self.env.reset()\n",
        "\n",
        "    for step in range(self.initial_memory_size):\n",
        "      if step % self.max_steps == 0:\n",
        "        state = self.env.reset()\n",
        "\n",
        "      vision = self._imagePreProcess_(state['frontview_image'])\n",
        "\n",
        "      action = np.random.randn(self.action_shape) # sample random action\n",
        "      next_state, reward, done, info = self.env.step(action)\n",
        "\n",
        "      vision_next = self._imagePreProcess_(next_state['frontview_image'])\n",
        "\n",
        "      vision = vision.squeeze(0)\n",
        "      vision_next = vision_next.squeeze(0)\n",
        "\n",
        "      obs = Batch({'vision': vision, 'robot0_eef_pos': state['robot0_eef_pos'],\n",
        "                            'robot0_eef_quat': state['robot0_eef_quat']})\n",
        "\n",
        "      obs_next = Batch({'vision': vision_next, 'robot0_eef_pos': next_state['robot0_eef_pos'],\n",
        "                            'robot0_eef_quat': next_state['robot0_eef_quat']})\n",
        "\n",
        "      action = torch.tensor(action, dtype=torch.float32)\n",
        "      action = action.squeeze(0)\n",
        "\n",
        "      batch = Batch({'obs': obs, 'act': action, 'rew': reward, 'obs_next': obs_next, 'terminated': done, 'truncated': done})\n",
        "\n",
        "      self.learner.pri_buffer.add(batch)\n",
        "      state = next_state\n",
        "\n",
        "    print('%d Data collected' % (self.initial_memory_size))\n",
        "\n",
        "  def train_model(self):\n",
        "    state = OrderedDict() \n",
        "    next_state = OrderedDict() \n",
        "    done = False \n",
        "    reward = 0 \n",
        "\n",
        "    for episode in range(self.num_episode):\n",
        "      state = self.env.reset()\n",
        "      episode_reward = 0\n",
        "      for t in range(self.max_steps):\n",
        "        vision = self._imagePreProcess_(state['frontview_image'])\n",
        "\n",
        "        robot_state = [\n",
        "            np.array(state['robot0_eef_pos'], dtype=np.float32).flatten(),\n",
        "            np.array(state['robot0_eef_quat'], dtype=np.float32).flatten()\n",
        "        ]\n",
        "\n",
        "        robot_state = np.concatenate(robot_state)\n",
        "        robot_state = torch.tensor(robot_state, dtype=torch.float32)\n",
        "\n",
        "        action = self.learner.get_action(vision, robot_state)\n",
        "        next_state, reward, done, info = self.env.step(action[0])\n",
        "\n",
        "        if len(self.reward_value)>=20:\n",
        "          self.reward_value.pop(0)\n",
        "          self.reward_value.append(reward)\n",
        "\n",
        "          vision_next = self._imagePreProcess_(next_state['frontview_image'])\n",
        "\n",
        "          vision = vision.squeeze(0)\n",
        "          vision_next = vision_next.squeeze(0)\n",
        "          obs = Batch({'vision': vision, 'robot0_eef_pos': state['robot0_eef_pos'],\n",
        "                              'robot0_eef_quat': state['robot0_eef_quat']})\n",
        "\n",
        "          obs_next = Batch({'vision_next': vision_next, 'robot0_eef_pos': next_state['robot0_eef_pos'],\n",
        "                              'robot0_eef_quat': next_state['robot0_eef_quat']})\n",
        "          action = torch.tensor(action, dtype=torch.float32)\n",
        "          action = action.squeeze(0)\n",
        "\n",
        "          batch = Batch({'obs': obs, 'act': action, 'rew': reward, 'obs_next': obs_next, 'terminated': done, 'truncated': done})\n",
        "          self.learner.pri_buffer.add(batch)\n",
        "          state = next_state\n",
        "          if any(done):\n",
        "            break\n",
        "\n",
        "        episode_reward += reward\n",
        "\n",
        "      if episode % 5 == 0:\n",
        "        self.learner.update()\n",
        "      self.episode_rewards.append(episode_reward)\n",
        "      if episode % self.evaluate_interval == 0:\n",
        "        self.averge_reward = np.mean(self.reward_value)\n",
        "        print(self.averge_reward)\n",
        "      if episode % 100 == 0:\n",
        "        print(\"Episode %d finished | Episode reward %f\" % (episode, episode_reward))\n",
        "      # if episode % self.save_every == 0:\n",
        "      #   checkpoint = {'episode': episode,\n",
        "      #   'actor_state_dict': self.learner.actor.state_dict(),\n",
        "      #   'target_actor_state_dict': self.learner.target_actor.state_dict(),\n",
        "      #   'actor_optimizer_state_dict': self.learner.actor_optimizer.state_dict(),\n",
        "      #   'critic_state_dict': self.learner.critic.state_dict(),\n",
        "      #   'target_critic_state_dict': self.learner.target_critic.state_dict(),\n",
        "      #   'critic_optimizer_state_dict': self.learner.critic_optimizer.state_dict()}\n",
        "      #   torch.save(checkpoint, '/content/drive/My Drive/check_point')\n",
        "      #   print('Model Saved')\n",
        "\n",
        "    self.env.close()\n",
        "\n",
        "  def train_tranformer(self):\n",
        "    for iter in range(self.max_iterators):\n",
        "      outputs = self.trainer.train_iteration(num_steps=self.num_steps_per_iter, iter_num=iter+1, print_logs=True)\n",
        "\n",
        "\n",
        "  def plot(self):\n",
        "    # Compute the moving average of cumulative rewards\n",
        "    moving_average = np.convolve(self.episode_rewards, np.ones(self.num_average_epidodes)/self.num_average_epidodes, mode='valid')\n",
        "    plt.plot(np.arange(len(moving_average)),moving_average)\n",
        "    plt.title('Average rewards in %d episodes' % self.num_average_epidodes)\n",
        "    plt.xlabel('episode')\n",
        "    plt.ylabel('rewards')\n",
        "    plt.show()\n",
        "\n",
        "  def test_model(self):\n",
        "    env = Stack('Panda')\n",
        "    state = env.reset()\n",
        "    self.learner.load_checkpoint('/content/drive/My Drive/check_point')\n",
        "\n",
        "    frames = []\n",
        "    for i in range(100):\n",
        "      frames.append(state['frontview_image'])  # Append the image to the frames list\n",
        "      obs = self._imagePreProcess_(state['frontview_image'])\n",
        "      action = self.learner.get_action(obs)\n",
        "\n",
        "      next_state, reward, done, info = env.step(action[0])\n",
        "      state = next_state\n",
        "\n",
        "      # if i == 50:\n",
        "      #   env.joint_pos = np.array([0.0, 0.5, 1.0, 1.3, 1.0, 1.0, 0.785])\n",
        "      #   env.object_pos = np.array([10,10,0])\n",
        "      #   env.object_quat = np.array([1, 0, 0])\n",
        "      #   state = env.reset()\n",
        "\n",
        "    imageio.mimwrite('robosuite_video.mp4', frames, fps=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvtZ6Z_taf3M"
      },
      "source": [
        "## Train agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilSkkI7apWmY",
        "outputId": "819fb03b-7aa6-4f58-b5ab-00e134b3d0be"
      },
      "outputs": [],
      "source": [
        "# drive.mount('/content/drive')\n",
        "\n",
        "serial_train = SerialModelTrain()\n",
        "serial_train.init_buffer()\n",
        "serial_train.train_model()\n",
        "serial_train.plot()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rgxoeCJaoK8"
      },
      "source": [
        "## Test Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRpDqBbitbMT"
      },
      "outputs": [],
      "source": [
        "serial_train.test_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYRnB8zG3QPL"
      },
      "outputs": [],
      "source": [
        "\"\"\" Load video and encode it in base64 format \"\"\"\n",
        "video_path = 'robosuite_video.mp4'\n",
        "video_data = open(video_path, 'rb').read()\n",
        "video_encoded = b64encode(video_data).decode()\n",
        "\n",
        "# Display video using HTML\n",
        "HTML(f\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"data:video/mp4;base64,{video_encoded}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
