{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEcuSBdHQpfd"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt\n",
        "\n",
        "\n",
        "# below fixes some bugs introduced by some recent Colab changes\n",
        "!mkdir -p /usr/share/vulkan/icd.d\n",
        "!wget -q https://raw.githubusercontent.com/haosulab/ManiSkill2/main/docker/nvidia_icd.json\n",
        "!wget -q https://raw.githubusercontent.com/haosulab/ManiSkill2/main/docker/10_nvidia.json\n",
        "!mv nvidia_icd.json /usr/share/vulkan/icd.d\n",
        "!mv 10_nvidia.json /usr/share/glvnd/egl_vendor.d/10_nvidia.json\n",
        "# dependencies\n",
        "#!pip install setuptools==65.5.0\n",
        "!apt-get install -y --no-install-recommends libvulkan-dev\n",
        "!pip install mani_skill2\n",
        "!pip install --upgrade --no-cache-dir gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aat2anESs9ap"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "import mani_skill2.envs\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import wandb\n",
        "wandb.login()\n",
        "\n",
        "# from collections import OrderedDict\n",
        "# import torch\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# from google.colab import drive\n",
        "\n",
        "\n",
        "# import parameters as params\n",
        "# from utils import save_checkpoint, load_checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Na-sLSqGs9ar"
      },
      "outputs": [],
      "source": [
        "env_id = \"PickCube-v0\"\n",
        "obs_mode = \"rgbd\"\n",
        "control_mode = \"pd_joint_delta_pos\"\n",
        "reward_mode = \"dense\"\n",
        "\n",
        "env = gym.make(env_id,\n",
        "               obs_mode=obs_mode,\n",
        "               reward_mode=reward_mode,\n",
        "               control_mode=control_mode,\n",
        "               enable_shadow=False)\n",
        "obs, _ = env.reset()\n",
        "print(\"Action Space:\", env.action_space)\n",
        "\n",
        "# take a look at the current state\n",
        "img = env.unwrapped.render_cameras()\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.title(\"Current State viewed through all RGB and Depth cameras\")\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = ManiSkill2Dataset(f\"demos/v0/rigid_body/{env_id}/trajectory.state.pd_ee_delta_pose.h5\")\n",
        "dataloader = DataLoader(dataset, batch_size=256, num_workers=0, pin_memory=True, drop_last=False, shuffle=False)\n",
        "obs, action = dataset[0]\n",
        "print(\"Observation:\", obs.shape)\n",
        "print(\"Action:\", action.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjX3oBFf5YpY"
      },
      "outputs": [],
      "source": [
        "from train import AgentTrainer\n",
        "from utils import convert_demonstration\n",
        "\n",
        "agent_trainer = AgentTrainer() \n",
        "\n",
        "\"\"\" Pre-train the policy using the demonstrations \"\"\"\n",
        "num_episode = 70000\n",
        "best_epoch_loss = np.inf\n",
        "pbar = tqdm(dataloader, total=num_episode)\n",
        "epoch = 0\n",
        "steps = 0\n",
        "\n",
        "for steps in range(num_episode):\n",
        "    epoch_loss = 0\n",
        "    for batch in dataloader:\n",
        "        steps += 1\n",
        "        goal, current, actions, video, proprioception = convert_demonstration(batch)\n",
        "        loss_val = agent_trainer.pre_train(goal, current, actions, video, proprioception)\n",
        "\n",
        "        # track the loss and print it\n",
        "        metrics = {\"train/train_loss\": loss_val}\n",
        "        wandb.log(metrics)\n",
        "\n",
        "        epoch_loss += loss_val\n",
        "        pbar.set_postfix(dict(loss=loss_val))\n",
        "        pbar.update(1)\n",
        "\n",
        "        # periodically save the policy\n",
        "        if steps % 10000 == 0: agent_trainer.save_checkpoint('check_point')\n",
        "        if steps >= num_episode: break\n",
        "\n",
        "    epoch_loss = epoch_loss / len(dataloader)\n",
        "\n",
        "    # save a new model if the average loss in an epoch has improved\n",
        "    if epoch_loss < best_epoch_loss:\n",
        "        best_epoch_loss = epoch_loss\n",
        "        agent_trainer.save_model(\"check_point\")\n",
        "\n",
        "    metrics = {\"epoch\": epoch,\n",
        "               \"train/loss_epoch\": epoch_loss}\n",
        "    wandb.log(metrics)\n",
        "    epoch += 1\n",
        "\n",
        "agent_trainer.save_checkpoint('check_point')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# class SerialModelTrain():\n",
        "#   def __init__(self):\n",
        "#     self.num_episode = 10\n",
        "#     self.initial_memory_size = 5\n",
        "#     self.episode_rewards = []\n",
        "#     self.num_average_epidodes = 100\n",
        "#     self.save_every = 100\n",
        "\n",
        "#     self.max_steps = 100\n",
        "#     self.envs = Stack('Panda')\n",
        "#     self.agent_trainer = AgentTrainer()\n",
        "\n",
        "#     self.evaluate_interval = 10\n",
        "#     self.reward_window = 20\n",
        "#     self.reward_values = [0]*self.reward_window\n",
        "#     self.averge_reward = 0\n",
        "\n",
        "#   def init_buffer(self):\n",
        "#     \"\"\" Initially, put the data into the replay buffer when an action with noise was taken \"\"\"\n",
        "#     state = OrderedDict()\n",
        "#     next_state = OrderedDict()\n",
        "#     done = False\n",
        "#     reward = 0\n",
        "\n",
        "#     state = self.env.reset()\n",
        "\n",
        "#     for step in range(self.initial_memory_size):\n",
        "#         if step % self.max_steps == 0:\n",
        "#           state = self.env.reset()\n",
        "\n",
        "#         action = np.random.randn(params.action_dim) # sample random action\n",
        "#         next_state, reward, done, info = self.env.step(action)\n",
        "\n",
        "#         # verify the following in the server\n",
        "#         vision = image_process(state['frontview_image'])\n",
        "#         vision_next = image_process(next_state['frontview_image'])\n",
        "#         action = torch.tensor(action, dtype=torch.float32)\n",
        "#         action = action.squeeze(0)\n",
        "\n",
        "#         self.agent_trainer.pri_buffer.store(state, action, reward, next_state, done)\n",
        "#         state = next_state\n",
        "\n",
        "#     print('%d Data collected' % self.initial_memory_size)\n",
        "\n",
        "\n",
        "#   def train_model(self):\n",
        "#     state = OrderedDict()\n",
        "#     next_state = OrderedDict()\n",
        "#     done = False\n",
        "#     reward = 0\n",
        "\n",
        "#     for episode in range(self.num_episode):\n",
        "#       state = self.env.reset()\n",
        "#       episode_reward = 0\n",
        "#       for t in range(self.max_steps):\n",
        "#         vision = image_process(state['frontview_image'])\n",
        "\n",
        "#         robot_state = [\n",
        "#             np.array(state['robot0_eef_pos'], dtype=np.float32).flatten(),\n",
        "#             np.array(state['robot0_eef_quat'], dtype=np.float32).flatten()\n",
        "#         ]\n",
        "\n",
        "#         robot_state = np.concatenate(robot_state)\n",
        "#         robot_state = torch.tensor(robot_state, dtype=torch.float32)\n",
        "\n",
        "#         action = self.agent_trainer.get_action(vision, robot_state)\n",
        "#         next_state, reward, done, info = self.env.step(action[0])\n",
        "\n",
        "#         if len(self.reward_value)>=20:\n",
        "#           self.reward_value.pop(0)\n",
        "#           self.reward_value.append(reward)\n",
        "\n",
        "#         # verify the following in the server\n",
        "#         vision_next = image_process(next_state['frontview_image'])\n",
        "#         vision = vision.squeeze(0)\n",
        "#         vision_next = vision_next.squeeze(0)\n",
        "#         action = torch.tensor(action, dtype=torch.float32)\n",
        "#         action = action.squeeze(0)\n",
        "\n",
        "#         self.agent_trainer.pri_buffer.store(state, action, reward, next_state, done)\n",
        "#         state = next_state\n",
        "#         if any(done):\n",
        "#           break\n",
        "\n",
        "#       episode_reward += reward\n",
        "\n",
        "#       if episode % 5 == 0:\n",
        "#         self.agent_trainer.update()\n",
        "#       self.episode_rewards.append(episode_reward)\n",
        "#       if episode % self.evaluate_interval == 0:\n",
        "#         self.averge_reward = np.mean(self.reward_value)\n",
        "#         print(self.averge_reward)\n",
        "#       if episode % 100 == 0:\n",
        "#         print(\"Episode %d finished | Episode reward %f\" % (episode, episode_reward))\n",
        "#       if episode % self.save_every == 0:\n",
        "#         self.agent_trainer.save_checkpoint(episode)\n",
        "\n",
        "#     self.env.close()\n",
        "\n",
        "#   def plot(self):\n",
        "#     # Compute the moving average of cumulative rewards\n",
        "#     moving_average = np.convolve(self.episode_rewards, np.ones(self.num_average_epidodes)/self.num_average_epidodes, mode='valid')\n",
        "#     plt.plot(np.arange(len(moving_average)),moving_average)\n",
        "#     plt.title('Average rewards in %d episodes' % self.num_average_epidodes)\n",
        "#     plt.xlabel('episode')\n",
        "#     plt.ylabel('rewards')\n",
        "#     plt.show()\n",
        "\n",
        "#   def test_model(self):\n",
        "#     env = Stack('Panda')\n",
        "#     states = env.reset()\n",
        "#     self.agent_trainer.load_checkpoint('/content/drive/My Drive/check_point')\n",
        "\n",
        "#     frames = []\n",
        "#     for i in range(100):\n",
        "#       frames.append(states['frontview_image'])  # Append the image to the frames list\n",
        "#       obs = image_process(states['frontview_image'])\n",
        "#       action = self.agent_trainer.get_action(obs)\n",
        "\n",
        "#       next_state, reward, done, info = env.step(action[0])\n",
        "#       state = next_state\n",
        "\n",
        "\n",
        "#     imageio.mimwrite('robosuite_video.mp4', frames, fps=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "init_memory_size = 1000\n",
        "observation, _ = env.reset()\n",
        "num_episode = 10000\n",
        "max_steps = 100\n",
        "save_every = 100\n",
        "episode_rewards = []\n",
        "\n",
        "for _ in range(init_memory_size):\n",
        "\n",
        "    vision = observation[\"image\"][\"base_camera\"][\"rgb\"] / 255.0\n",
        "    proprioception = observation[\"state\"]\n",
        "\n",
        "    action = agent_trainer.get_action(goal, vision, proprioception)\n",
        "    next_observation, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "    next_vision = next_observation[\"image\"][\"base_camera\"][\"rgb\"] / 255.0\n",
        "    next_proprioception = next_observation[\"state\"]\n",
        "    agent_trainer.pri_buffer.store(goal, reward, vision, next_vision, next_proprioception, done)\n",
        "\n",
        "    observation = next_observation\n",
        "\n",
        "    if done:\n",
        "        observation, _ = env.reset()\n",
        "\n",
        "\n",
        "for epoch in range(num_episode):\n",
        "    observation, _ = env.reset()\n",
        "    episode_reward = 0\n",
        "    for t in range(max_steps):\n",
        "\n",
        "        vision = observation[\"image\"][\"base_camera\"][\"rgb\"] / 255.0\n",
        "        proprioception = observation[\"state\"]\n",
        "        action = agent_trainer.get_action(goal, vision, proprioception)\n",
        "        next_observation, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "        agent_trainer.pri_buffer.store(goal, reward, vision, next_vision, next_proprioception, done)\n",
        "\n",
        "        observation = next_observation\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    episode_reward += reward\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        agent_trainer.fine_tune()\n",
        "\n",
        "    episode_rewards.append(episode_reward)\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        metrics = {\"train/episode\": epoch, \n",
        "                   \"train/episode_reward\": episode_reward}\n",
        "        wandb.log(metrics)\n",
        "    if epoch % save_every == 0:\n",
        "        agent_trainer.save_checkpoint(\"check_point\")\n",
        "\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvtZ6Z_taf3M"
      },
      "source": [
        "## Train agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilSkkI7apWmY"
      },
      "outputs": [],
      "source": [
        "# # drive.mount('/content/drive')\n",
        "\n",
        "# serial_train = SerialModelTrain()\n",
        "# serial_train.init_buffer()\n",
        "# serial_train.train_model()\n",
        "# serial_train.plot()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rgxoeCJaoK8"
      },
      "source": [
        "## Test Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRpDqBbitbMT"
      },
      "outputs": [],
      "source": [
        "# serial_train.test_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYRnB8zG3QPL"
      },
      "outputs": [],
      "source": [
        "# \"\"\" Load video and encode it in base64 format \"\"\"\n",
        "# video_path = 'robosuite_video.mp4'\n",
        "# video_data = open(video_path, 'rb').read()\n",
        "# video_encoded = b64encode(video_data).decode()\n",
        "\n",
        "# # Display video using HTML\n",
        "# HTML(f\"\"\"\n",
        "# <video width=\"640\" height=\"480\" controls>\n",
        "#   <source src=\"data:video/mp4;base64,{video_encoded}\" type=\"video/mp4\">\n",
        "# </video>\n",
        "# \"\"\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
